# Architecture

## System Design

The voice bot is a real-time audio pipeline built as a **FastAPI WebSocket server** that bridges **Twilio Media Streams** to an AI stack of **Deepgram** (STT/TTS) and **OpenAI GPT-4o-mini** (dialogue generation). When a test call is initiated, Twilio places an outbound call to the target number and opens a bidirectional WebSocket to our server. Inbound audio (the AI agent's voice) arrives as base64-encoded μ-law frames, which we decode and stream directly to Deepgram's Nova-2 model over a persistent WebSocket for real-time transcription. When Deepgram's endpointing detects the agent has finished speaking, the accumulated transcript is sent to GPT-4o-mini, which is prompted with a scenario-specific patient persona (e.g., an elderly confused patient, a non-native English speaker requesting a refill). The LLM's response is then streamed through Deepgram's Aura TTS engine, which outputs μ-law audio natively—eliminating any format conversion overhead. These audio chunks are sent back through the Twilio WebSocket at 20ms intervals to maintain natural playback cadence, with barge-in support that clears the audio buffer if the agent interrupts.

## Key Design Choices

I chose **Deepgram over Twilio's built-in `<Gather>` verb** because Media Streams + Deepgram gives true streaming STT with sub-300ms latency, whereas `<Gather>` waits for complete utterances and round-trips through webhooks, adding 3-5 seconds of dead air that would make conversations unnatural and mask real bugs. I chose **Deepgram for both STT and TTS** because both natively support μ-law 8kHz encoding (Twilio's wire format), eliminating audio transcoding that would add latency and complexity. **GPT-4o-mini** was selected over larger models because patient dialogue generation doesn't require deep reasoning—it needs fast, natural responses, and the smaller model keeps per-call cost under $0.01 while maintaining conversational quality. The **scenario system** uses detailed persona prompts rather than rigid scripts because real patients are unpredictable; the LLM naturally handles unexpected agent questions while staying in character, which is more effective at discovering edge cases than scripted flows. Finally, the **`[END_CALL]` sentinel** in LLM output provides a clean mechanism for the bot to end calls naturally without requiring complex conversation-state tracking.
